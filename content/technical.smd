---
.title = "Technical Approach",
.date = @date("1990-01-01T00:00:00"),
.author = "Sample Author",
.layout = "index.shtml",
.draft = false,
--- 

## How the Pipeline Works

The pipeline processes audio through five stages:

```
┌─────────────┐    ┌─────────────┐    ┌─────────────┐    ┌─────────────┐    ┌─────────────┐
│   Cleaning  │───▶│ Degradation │───▶│  Feature    │───▶│    MOS      │───▶│  Business   │
│             │    │             │    │ Extraction  │    │ Prediction  │    │  Metrics    │
└─────────────┘    └─────────────┘    └─────────────┘    └─────────────┘    └─────────────┘
      │                  │                  │                  │                  │
   16kHz            Codec/Noise        60+ features       XGBoost           Annual cost
   normalize        Bandwidth          Acoustic           R² = 0.54         per employee
   trim silence     Network            Prosodic
                                       Perceptual
```

## Stage 1: Audio Cleaning

All audio is standardized to a common format before processing:

- **Sample rate**: 16kHz (balances quality with processing speed)
- **Channels**: Mono (stereo averaged)
- **Normalization**: Peak normalized to -3dB
- **Silence trimming**: Leading/trailing silence removed

This ensures consistent feature extraction regardless of source.

## Stage 2: Degradation Simulation

To understand how quality degrades, I simulate real-world impairments. The pipeline supports four degradation types that can be combined:

### Codec Compression

Simulates lossy audio codecs used in VoIP and streaming:

```python
# From src/speech_quality/degradation/codec.py
CODEC_CONFIGS = {
    "mp3_64k": {"type": "mp3", "bitrate_kbps": 64},    # Low quality streaming
    "mp3_128k": {"type": "mp3", "bitrate_kbps": 128},  # Standard streaming
    "opus_16k": {"type": "opus", "bitrate_kbps": 16},  # Extreme VoIP compression
    "opus_32k": {"type": "opus", "bitrate_kbps": 32},  # Standard VoIP
    "amr_nb": {"type": "amr-nb", "bitrate_kbps": 12.2}, # Mobile phone quality
}
```

The codec degrader encodes and decodes audio to capture compression artifacts:

```python
def apply_codec_degradation(self, input_path, output_path, codec_config):
    """Apply codec compression artifacts to audio."""
    # Load → Encode to lossy format → Decode back to WAV
    # This captures the quality loss from compression
```

### Bandwidth Limiting

Applies frequency filtering to simulate telephone and VoIP constraints:

```python
# From src/speech_quality/degradation/bandwidth.py
BANDWIDTH_CONFIGS = {
    "telephone": {"low_cutoff_hz": 300, "high_cutoff_hz": 3400},   # PSTN
    "voip_narrow": {"low_cutoff_hz": 50, "high_cutoff_hz": 7000},  # G.729
    "voip_wide": {"low_cutoff_hz": 50, "high_cutoff_hz": 7500},    # G.722.1
}
```

Removing high frequencies makes speech sound muffled—consonants like 's' and 'f' become harder to distinguish.

### Noise Addition

Adds environmental noise at controlled signal-to-noise ratios:

```python
# From src/speech_quality/degradation/noise.py
NOISE_CONFIGS = {
    "white_5db": {"type": "white", "snr_db": 5},    # Very noisy
    "white_15db": {"type": "white", "snr_db": 15},  # Moderate noise
    "babble_10db": {"type": "babble", "snr_db": 10}, # Background conversations
    "office_20db": {"type": "ambient", "snr_db": 20}, # Office hum
}
```

### Network Impairments

Simulates VoIP network issues:

```python
# From src/speech_quality/degradation/network.py
NETWORK_CONFIGS = {
    "packet_loss_1pct": {"type": "packet_loss", "loss_rate": 0.01},
    "packet_loss_5pct": {"type": "packet_loss", "loss_rate": 0.05},
    "jitter_20ms": {"type": "jitter", "jitter_ms": 20},
    "jitter_50ms": {"type": "jitter", "jitter_ms": 50},
}
```

### Degradation Chains

Real-world degradation combines multiple factors. A VoIP call might experience codec compression, bandwidth limiting, AND packet loss simultaneously.

```python
# From src/speech_quality/degradation/pipeline.py
def apply_degradation_chain(
    self,
    input_path: Path,
    output_path: Path,
    degradation_types: list[str],  # e.g., ["codec", "bandwidth", "noise"]
    codec_config: str = None,
    bandwidth_config: str = None,
    noise_config: str = None,
):
    """Apply multiple degradations in sequence.

    Each degradation's output becomes the next one's input,
    accumulating effects realistically.
    """
    current_file = input_path

    for deg_type in degradation_types:
        intermediate_file = tmp_dir / f"step_{step}.wav"

        if deg_type == "codec":
            self.codec_degrader.apply(current_file, intermediate_file, codec_config)
        elif deg_type == "bandwidth":
            self.bandwidth_limiter.apply(current_file, intermediate_file, bandwidth_config)
        # ... etc

        current_file = intermediate_file

    shutil.copy(current_file, output_path)
```

## Stage 3: Feature Extraction

I extract 60+ features across three categories:

### Acoustic Features (via librosa)

Spectral characteristics that capture how energy is distributed across frequencies:

```python
# From src/speech_quality/features/extractor.py
def extract_acoustic_features(self, audio: np.ndarray, sr: int) -> dict:
    """Extract acoustic features using librosa."""
    features = {}

    # MFCCs - mel-frequency cepstral coefficients
    # Captures the "shape" of the frequency spectrum
    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)
    features["mfcc_mean"] = np.mean(mfccs, axis=1).tolist()
    features["mfcc_std"] = np.std(mfccs, axis=1).tolist()

    # Spectral centroid - "brightness" of the sound
    centroid = librosa.feature.spectral_centroid(y=audio, sr=sr)[0]
    features["spectral_centroid_mean"] = float(np.mean(centroid))

    # Spectral bandwidth - spread of frequencies
    bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr)[0]
    features["spectral_bandwidth_mean"] = float(np.mean(bandwidth))

    # Zero crossing rate - indicator of noisiness
    zcr = librosa.feature.zero_crossing_rate(audio)[0]
    features["zero_crossing_rate_mean"] = float(np.mean(zcr))

    return features
```

### Prosodic Features (via Parselmouth/Praat)

Voice characteristics that relate to speech production quality:

```python
def extract_prosodic_features(self, audio_path: Path) -> dict:
    """Extract prosodic features using Parselmouth (Praat)."""
    features = {}

    snd = parselmouth.Sound(str(audio_path))

    # Pitch (F0) - fundamental frequency of voice
    pitch = snd.to_pitch()
    pitch_values = pitch.selected_array["frequency"]
    pitch_values = pitch_values[pitch_values > 0]  # Remove unvoiced

    features["f0_mean"] = float(np.mean(pitch_values))
    features["f0_range"] = float(np.max(pitch_values) - np.min(pitch_values))

    # Jitter - cycle-to-cycle variation in pitch
    # Higher jitter = rougher voice quality
    point_process = parselmouth.praat.call(snd, "To PointProcess (periodic, cc)", 75, 600)
    jitter = parselmouth.praat.call(point_process, "Get jitter (local)", 0, 0, 0.0001, 0.02, 1.3)
    features["jitter_local"] = float(jitter)

    # Shimmer - cycle-to-cycle variation in amplitude
    # Higher shimmer = more breathiness/hoarseness
    shimmer = parselmouth.praat.call(
        [snd, point_process], "Get shimmer (local)", 0, 0, 0.0001, 0.02, 1.3, 1.6
    )
    features["shimmer_local"] = float(shimmer)

    # HNR - Harmonics-to-Noise Ratio
    # Higher HNR = cleaner voice signal
    harmonicity = snd.to_harmonicity()
    features["hnr_mean"] = float(np.mean(harmonicity.values[harmonicity.values != -200]))

    return features
```

### Perceptual Features

Derived features that relate to human perception:

```python
def extract_perceptual_features(self, audio: np.ndarray, sr: int) -> dict:
    """Extract perceptual features."""
    features = {}

    # Effective bandwidth - frequency range with significant energy
    fft = np.fft.rfft(audio)
    power_db = 10 * np.log10(np.abs(fft) ** 2 + 1e-10)
    freqs = np.fft.rfftfreq(len(audio), 1 / sr)
    above_threshold = power_db > (np.max(power_db) - 40)
    features["effective_bandwidth_hz"] = float(
        freqs[above_threshold].max() - freqs[above_threshold].min()
    )

    # Spectral flatness - how "noisy" vs "tonal" the signal is
    flatness = librosa.feature.spectral_flatness(y=audio)[0]
    features["spectral_flatness_mean"] = float(np.mean(flatness))

    # Dynamic range
    features["dynamic_range_db"] = float(
        20 * np.log10(np.max(np.abs(audio)) / (np.mean(np.abs(audio)) + 1e-10))
    )

    return features
```

### Feature Summary

| Category | Features | Source |
|----------|----------|--------|
| Acoustic | MFCCs, spectral centroid/bandwidth/rolloff, ZCR, RMS | librosa |
| Prosodic | F0, formants (F1-F3), jitter, shimmer, HNR | Parselmouth |
| Perceptual | Effective bandwidth, spectral flatness, dynamic range | Custom |

## Stage 4: Model Training

Features feed into regression models that predict MOS:

```python
# From src/speech_quality/models/mos_regression.py
class MOSPredictor:
    """MOS prediction using multiple regression models."""

    def __init__(self, model_type: str = "xgboost"):
        self.model_type = model_type
        self.model = self._create_model(model_type)
        self.scaler = StandardScaler()

    def _create_model(self, model_type: str):
        if model_type == "xgboost":
            return xgb.XGBRegressor(
                n_estimators=1000,
                max_depth=6,
                learning_rate=0.1,
                early_stopping_rounds=50,
            )
        elif model_type == "lightgbm":
            return lgb.LGBMRegressor(...)
        elif model_type == "random_forest":
            return RandomForestRegressor(n_estimators=200, max_depth=15)
        elif model_type == "linear":
            return LinearRegression()

    def train(self, X_train, y_train, X_val=None, y_val=None):
        """Train with optional validation set for early stopping."""
        X_train_scaled = self.scaler.fit_transform(X_train)

        if self.model_type in ["xgboost", "lightgbm"] and X_val is not None:
            X_val_scaled = self.scaler.transform(X_val)
            self.model.fit(
                X_train_scaled, y_train,
                eval_set=[(X_val_scaled, y_val)],
            )
        else:
            self.model.fit(X_train_scaled, y_train)
```

## Data Storage: DuckDB

All features are stored in DuckDB for efficient querying:

```sql
-- Query features with MOS scores for model training
SELECT f.*, m.mos_score
FROM features f
JOIN mos_scores m ON f.file_id = m.file_id
WHERE m.source = 'ground_truth'
```

DuckDB was chosen because:
- Handles large datasets efficiently (column-oriented storage)
- SQL interface for flexible analysis
- Embedded (no server to manage)
- Python-native integration

---
